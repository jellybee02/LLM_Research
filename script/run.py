import numpy as np, pandas as pd, warnings, os, openai, json
from tqdm.auto import tqdm
from openai import OpenAI
warnings.filterwarnings('ignore')
import requests
from typing import List, Tuple, Union
from langchain_ollama.embeddings import OllamaEmbeddings
import pickle
from sklearn.metrics.pairwise import cosine_similarity
from haversine import haversine, Unit


with open('../data/user_input_content.json', "rb") as f:
    input_content = json.load(f)
df = pd.read_csv("../data/prep3.csv", encoding='cp949')
key = open('../../../api_key.txt','r')
api_key = key.read()
openai.api_key = api_key
with open('../data/encoded_tool_name2.pkl', "rb") as f:
    all_tool_encoded_array = pickle.load(f)
use_col = ['ê³µêµ¬ ì´ë¦„', 'ê³¼ê¸ˆê¸°ì¤€', 'ìˆ˜ëŸ‰','ëŒ€ì—¬ì¥ì†Œëª…', 'ìƒì„¸ì£¼ì†Œ','ì „í™”ë²ˆí˜¸', 'í‰ì¼ì˜¤í”ˆì‹œê°„', 'í‰ì¼í´ë¡œì¦ˆì‹œê°„', 'ìƒì„±ì¼ì‹œ', 'ìš”ê¸ˆ']

    
    

class OllamaSentenceTransformer():
    def __init__(
            self,
            *args,
            **kargs,
            ) -> None:
                # self.base_url = kargs.get()
                self.model = kargs.get("model","bge-m3")
                self.embedding_model = OllamaEmbeddings(
                            model = self.model,
                            # base_url = self.base_rul,
                        )
                        
    
    def encode(
            self,
            documents:Union[str, List[str], np.ndarray],
            *args,
            **kargs,
        )-> np.ndarray:
        if isinstance(documents, str):
            document_embeddings = self.embedding_model.embed_query(documents)
            return np.array(document_embeddings)
        
        if isinstance(documents, np.ndarray):
            documents = documents.tolist()
        
        document_embeddings = [self.embedding_model.embed_query(s) for s in documents]
        return np.array(document_embeddings)
        




sentence_transformer = OllamaSentenceTransformer()



def get_lat_lon(location_name):
    url = 'https://nominatim.openstreetmap.org/search'
    params = {
        'q': location_name,
        'format': 'json'
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (compatible; ChatGPT-Example/1.0; +http://yourdomain.com)'
    }

    response = requests.get(url, params=params, headers=headers)

    # ì‘ë‹µ í™•ì¸
    if response.status_code != 200:
        print(f"ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        print("ì‘ë‹µ ë‚´ìš©:", response.text)
        return None

    try:
        data = response.json()
        if data:
            lat = data[0]['lat']
            lon = data[0]['lon']
            return float(lat), float(lon)
        else:
            print("ìœ„ì¹˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    except ValueError as e:
        print("JSON íŒŒì‹± ì˜¤ë¥˜:", e)
        print("ì‘ë‹µ ë‚´ìš©:", response.text)
        return None


def compare_cosim(all_asset_encoded_val:np.array, asset_info:np.array) -> float:
    cos_sim = cosine_similarity(all_asset_encoded_val, [asset_info])
    return cos_sim


encoded_tool_name = sentence_transformer.encode(input_content['tool_name'])
location_name = input_content['location']
spot_location = get_lat_lon(location_name)
df['tool_sim_result'] = compare_cosim(all_tool_encoded_array, encoded_tool_name)
df_s = df[df['tool_sim_result']>0.9]
df_s['Distance(Km)'] = df_s[["ìœ„ë„","ê²½ë„"]].apply(lambda x : round(haversine((spot_location[0], spot_location[1]), (x['ìœ„ë„'], x['ê²½ë„']), unit=Unit.KILOMETERS),3), axis=1)
df_s.sort_values(by='Distance(Km)', ascending=True, inplace=True)



client = OpenAI(api_key=api_key)
def invoke(question, info = None, model="gpt-4o", temperature=0.7):
    response = client.chat.completions.create(
        model=model,
        messages = [
                    {"role": "system", 
                        "content": 
                            
                        
                            f"""ë„ˆëŠ” ì‚¬ëŒë“¤ì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì£¼ëŠ” ë„ìš°ë¯¸ì•¼.
                        ë„ˆëŠ” ì„œìš¸ì‹œì—ì„œ ìš´ì˜í•˜ëŠ” ëŒ€ì—¬ ê³µêµ¬ ì°¾ê¸° ì •ë³´ ì‹œìŠ¤í…œì´ì•¼. 
                        ê´€ë ¨ ì •ë³´ëŠ” {info}ì™€ ê°™ì•„. 
                        ì´ì •ë³´ë¥¼ í†µí•´ì„œ ì‚¬ìš©ìì˜ ë‹µë³€ì— ì¹œì ˆí•´ ë‹µí•´ì¤˜.
                        """ if info != None else
                            f"""ë„ˆëŠ” ì‚¬ëŒë“¤ì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ë‹µí•´ì£¼ëŠ” ë„ìš°ë¯¸ì•¼.
                        ë„ˆëŠ” ì„œìš¸ì‹œì—ì„œ ìš´ì˜í•˜ëŠ” ëŒ€ì—¬ ê³µêµ¬ ì°¾ê¸° ì •ë³´ ì‹œìŠ¤í…œì´ì•¼. 
                        ì‚¬ìš©ìì˜ ë‹µë³€ì— ì¹œì ˆí•´ ë‹µí•´ì¤˜.
                        """
                        },
                    {"role": "user", 
                    "content": question}
                ],
        temperature=temperature,
        # stream=True
    )
    # print("ğŸ’¬ GPT ì‘ë‹µ")
    # print("")
    # for chunk in response.choices[0].message.content:
    #     print(chunk, end='', flush=True)
    
    
    
    return response.choices[0].message.content

question = f"""ë‚˜ëŠ” {input_content['tool_name']}ì„(ë¥¼) ë¹Œë¦¬ê³  ì‹¶ì–´. 
            ê·¸ë¦¬ê³  ì´ ë„êµ¬ë¥¼ ì´ìš©í•´ì„œ í•˜ë ¤ëŠ” ì‘ì—…ì€ '{input_content['job_content']}' ì´ì•¼. 
            í•´ë‹¹ ì‘ì—…ì„ í•˜ë©´ì„œ ê°™ì´ ë¹Œë¦¬ë©´ ì¢‹ì€ ê³µêµ¬ë„ í•¨ê»˜ ì•Œë ¤ì¤˜"""



answer = invoke(question)
print("ê°€ê¹Œìš´ ëŒ€ì—¬ì†Œ : ", df_s[use_col].head(3).T.to_dict())
print("ğŸ’¬ GPT ì‘ë‹µ:", answer)










